{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a597969d-e598-451a-ab0e-e095fbb3e4df",
   "metadata": {
    "id": "a597969d-e598-451a-ab0e-e095fbb3e4df"
   },
   "source": [
    "### Основные этапы автоматической обработки естественного текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321f3d3-3d93-4282-becb-0f65ffc83a27",
   "metadata": {
    "id": "2321f3d3-3d93-4282-becb-0f65ffc83a27"
   },
   "source": [
    "Для чего мы с вами вообще изучаем питон? Потому что у него простой и максимально близкий к человеческому языку синтаксис: именно поэтому инженеры-программисты создали огромное количество разных библиотек в питоне, не только лингвистических, но в самых разных областях (например, есть для геофизиков модули). Более того, именно питон &ndash; основной язык для машинного и глубокого обучения, не в последнюю очередь потому, что позволяет обрабатывать данные и анализировать их в процессе (в тетрадках .ipynb), что очень важно для дата саентистов. Для теоретических лингвистов тоже очень удобно. \n",
    "\n",
    "На питоне написано большое количество лингвистических модулей и библиотек; существует несколько крупных библиотек, которые содержат в себе набор инструментов. Самая старая и известная &ndash; это NLTK; очень на нее похожа TextBlob. Более новая, модная, быстрая и на нейронках &ndash; spaCy (написана на Cython). Эти три вышеназванные &ndash; мультиязычные. Создаются и библиотеки для конкретных языков. Для русского языка &ndash; это natasha и DeepPavlov(МФТИ). \n",
    "\n",
    "Крупные библиотеки (NLTK и в основном spaCy в связке с udpipe) мы посмотрим в следующий раз, а пока сконцентрируемся на конкретных этапах обработки текста + отдельных инструментах, не входящих (или входящих) в большие библиотеки. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af01cc-9cd3-45c7-b63d-3a7e55535e80",
   "metadata": {
    "id": "c0af01cc-9cd3-45c7-b63d-3a7e55535e80"
   },
   "source": [
    "#### Установка примерно всех библиотек, которые нам понадобятся"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c746b-14fc-43b0-b4f3-5e6046515524",
   "metadata": {
    "id": "c63c746b-14fc-43b0-b4f3-5e6046515524"
   },
   "source": [
    "Команды, выделенные курсивом, выполняются в консоли, остальные в интерактивной среде питона. В Colaboratory консольные команды можно запускать в обычных ячейках с восклицательным знаком:\n",
    "\n",
    "    !pip install mymodule\n",
    "\n",
    "**NLTK**\n",
    "\n",
    "*pip install nltk*\n",
    "\n",
    "    import nltk\n",
    "    nltk.download()\n",
    "    \n",
    "В появившемся окне загрузки выбрать all и загрузить. Может быть довольно долго! \n",
    "\n",
    "*Colab*: предустановлен, но дополнительные штуки не загружены (с их загрузкой могут быть проблемы &ndash; в колабе нет графического окна для нее). \n",
    "\n",
    "**spaCy**\n",
    "\n",
    "*pip install spacy*\n",
    "\n",
    "*python -m spacy download _modelname_*  # список возможных моделей и сама команда доступны на [официальном сайте спейси](https://spacy.io/usage/models)\n",
    "\n",
    "Нам еще понадобится [обертка udpipe для spacy](https://spacy.io/universe/project/spacy-udpipe):\n",
    "\n",
    "*pip install spacy-udpipe*\n",
    "\n",
    "*Colab*: библиотека spacy предустановлена, может понадобиться загрузить модель (т.е. использовать только вторую команду)\n",
    "\n",
    "**natasha (подмодуль razdel)**\n",
    "\n",
    "*pip install razdel* \n",
    "\n",
    "Можете и саму наташу установить, если интересно: там много разных инструментов. [Наташин гитхаб](https://github.com/natasha/natasha)\n",
    "\n",
    "*Colab*: устанавливается без проблем. \n",
    "\n",
    "**DeepPavlov (ru_sent_tokenize)**\n",
    "\n",
    "*pip install rusenttokenize*\n",
    "\n",
    "Можно и весь DeepPavlov установить, но у него могут возникнуть проблемы с совместимостью (по крайней мере, раньше он конфликтовал со spacy из-за версий tensorflow &ndash; библиотеки, на которой написаны их нейронные модели). \n",
    "\n",
    "*Colab*: устанавливается без проблем. \n",
    "\n",
    "**pymorphy2**\n",
    "\n",
    "*pip install pymorphy2*\n",
    "\n",
    "*pip install -U pymorphy2-dicts-ru*  # необязательно: для обновления словаря\n",
    "\n",
    "*Colab*: устанавливается без проблем. \n",
    "\n",
    "**Mystem**\n",
    "\n",
    "*pip install git+https://github.com/nlpub/pymystem3 *\n",
    "\n",
    "Может козлить во время установки. Обязательно устанавливайте последнюю версию через git (не stable), потому что только в ней есть некоторые важные функции. \n",
    "\n",
    "*Colab*: гарантированно не работает. \n",
    "\n",
    "**rnnmorph**\n",
    "\n",
    "*pip install rnnmorph*\n",
    "\n",
    "Тоже может козлить во время установки (вы сами видели...). Иногда, если плохо установился и не работает, приходится его удалять командой pip uninstall rnnmorph (обязательно в консоли от имени администратора!). Периодически, если обновляется версия tensorflow, может перестать работать &ndash; но Илья Гусев обычно вскоре обновляет и свой парсер, так что достаточно следить за новостями в [его гитхабе](https://github.com/IlyaGusev/rnnmorph).\n",
    "\n",
    "*Colab*: обычно работает без проблем + возможно использовать видеокарточки гугла для ускорения (в меню runtime есть change runtime type &ndash; позволяет выбрать hardware accelerator: GPU &ndash; это видеокарта, TPU &ndash; это Tensor Processing Unit, специальная разработка гугла для работы нейронных сетей). При этом может выдавать ворнинги (не страшные). \n",
    "\n",
    "**pyconll**\n",
    "\n",
    "*pip install pyconll*\n",
    "\n",
    "*Colab*: устанавливается без проблем. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb1c89a-77d4-4126-914c-8df0f6f4de4c",
   "metadata": {
    "id": "6bb1c89a-77d4-4126-914c-8df0f6f4de4c"
   },
   "source": [
    "#### Сегментация по предложениям\n",
    "\n",
    "Обычно разделение по предложениям делается до токенизации, потому что так технически проще.\n",
    "\n",
    "    Это в том числе связано с внутренним устройством сентенайзеров - razdel, например, сперва отыскивает все возможные разделители (delimiters: точки, ?, !, многоточия и т.п.) с помощью функции re.finditer, а потом идет циклом по найденному, с помощью методов start() и end() получает индексы разделителя в исходной строке, берет окно в некоторое количество символов слева и справа и проверяет, что слева, а что справа: есть ли слева аббревиатура (у него есть словарь с ними), есть ли справа заглавная буква и т.д. Брать окно, если у нас уже список токенов, затруднительно: плюс ко всему, токенизатор тоже по-своему обрабатывает аббревиатуры, скорее всего. \n",
    "\n",
    "Самый базовый сентенайзер &ndash; это NLTK. Также есть сентенайзер razdel (он работает быстрее всех) и в DeepPavlov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff208f5b-5636-4673-9c83-90ad27a57d0d",
   "metadata": {
    "id": "ff208f5b-5636-4673-9c83-90ad27a57d0d"
   },
   "outputs": [],
   "source": [
    "raw = '''Пердикка II (др.-греч. Περδίκκας Β΄ της Μακεδονίας) — македонский царь, правивший в 454—413 годах до н. э. После смерти Александра I среди его сыновей возник междоусобный конфликт, победителем из которого вышел Пердикка. На момент его воцарения Македония представляла собой отсталое государство, которому угрожала опасность завоевания как со стороны Афинского морского союза на юге, так и Одрисского царства на севере. На первых порах Пердикка был вынужден всеми силами избегать открытого вооружённого противостояния и лишь наблюдать за появлением множества греческих колоний на своих границах. С началом Пелопоннесской войны македонский царь с максимальной выгодой для государства использовал запутанные отношения между греческими полисами на Халкидиках, Афинами, Спартой и Коринфом. Пердикка не менее десяти раз заключал и расторгал союзы с основными участниками войны.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d0c756-a60c-4b8d-a761-5946df635459",
   "metadata": {
    "id": "66d0c756-a60c-4b8d-a761-5946df635459",
    "outputId": "4509f4f8-cc44-4c34-a58d-5dd4d347bccf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Пердикка II (др.-греч.',\n",
       " 'Περδίκκας Β΄ της Μακεδονίας) — македонский царь, правивший в 454—413 годах до н. э. После смерти Александра I среди его сыновей возник междоусобный конфликт, победителем из которого вышел Пердикка.',\n",
       " 'На момент его воцарения Македония представляла собой отсталое государство, которому угрожала опасность завоевания как со стороны Афинского морского союза на юге, так и Одрисского царства на севере.',\n",
       " 'На первых порах Пердикка был вынужден всеми силами избегать открытого вооружённого противостояния и лишь наблюдать за появлением множества греческих колоний на своих границах.',\n",
       " 'С началом Пелопоннесской войны македонский царь с максимальной выгодой для государства использовал запутанные отношения между греческими полисами на Халкидиках, Афинами, Спартой и Коринфом.',\n",
       " 'Пердикка не менее десяти раз заключал и расторгал союзы с основными участниками войны.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sents = sent_tokenize(raw)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675852f-f7a3-4be0-aa73-23fd1d202bc8",
   "metadata": {
    "id": "0675852f-f7a3-4be0-aa73-23fd1d202bc8",
    "outputId": "018c8fe6-d735-4e49-8168-2d35eb6fbff6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Пердикка II (др.-греч. Περδίκκας Β΄ της Μακεδονίας) — македонский царь, правивший в 454—413 годах до н. э.',\n",
       " 'После смерти Александра I среди его сыновей возник междоусобный конфликт, победителем из которого вышел Пердикка.',\n",
       " 'На момент его воцарения Македония представляла собой отсталое государство, которому угрожала опасность завоевания как со стороны Афинского морского союза на юге, так и Одрисского царства на севере.',\n",
       " 'На первых порах Пердикка был вынужден всеми силами избегать открытого вооружённого противостояния и лишь наблюдать за появлением множества греческих колоний на своих границах.',\n",
       " 'С началом Пелопоннесской войны македонский царь с максимальной выгодой для государства использовал запутанные отношения между греческими полисами на Халкидиках, Афинами, Спартой и Коринфом.',\n",
       " 'Пердикка не менее десяти раз заключал и расторгал союзы с основными участниками войны.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import sentenize\n",
    "\n",
    "sents = [s.text for s in sentenize(raw)]  \n",
    "'''функция возвращает список экземпляров спец.класса, которые хранят в себе не\n",
    "только текст предложений, но и индексы их начала и конца в исходной строке. \n",
    "Это принцип всего раздела, его токенизатор устроен точно так же. \n",
    "'''\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a376abb-e130-4711-9eb0-5a855fa6b102",
   "metadata": {
    "id": "1a376abb-e130-4711-9eb0-5a855fa6b102",
    "outputId": "e34e9986-d460-4711-e7f2-991ab61d3ad8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Пердикка II (др.-греч. Περδίκκας Β΄ της Μακεδονίας) — македонский царь, правивший в 454—413 годах до н. э.',\n",
       " 'После смерти Александра I среди его сыновей возник междоусобный конфликт, победителем из которого вышел Пердикка.',\n",
       " 'На момент его воцарения Македония представляла собой отсталое государство, которому угрожала опасность завоевания как со стороны Афинского морского союза на юге, так и Одрисского царства на севере.',\n",
       " 'На первых порах Пердикка был вынужден всеми силами избегать открытого вооружённого противостояния и лишь наблюдать за появлением множества греческих колоний на своих границах.',\n",
       " 'С началом Пелопоннесской войны македонский царь с максимальной выгодой для государства использовал запутанные отношения между греческими полисами на Халкидиках, Афинами, Спартой и Коринфом.',\n",
       " 'Пердикка не менее десяти раз заключал и расторгал союзы с основными участниками войны.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rusenttokenize import ru_sent_tokenize\n",
    "\n",
    "sents = ru_sent_tokenize(raw)\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab5169-906a-43c7-b733-395294cf8c41",
   "metadata": {
    "id": "b9ab5169-906a-43c7-b733-395294cf8c41"
   },
   "source": [
    "#### Токенизация\n",
    "\n",
    "Свои токенизаторы есть у nltk, razdel и spacy (есть токенизаторы и в DeepPavlov, но библиотека при установке конфликтует с другими библиотеками, поэтому для простоты я его обычно не показываю &ndash; хотя вы можете сами посмотреть... [тут](http://docs.deeppavlov.ai/en/master/apiref/models/tokenizers.html)). \n",
    "\n",
    "Также свои токенизаторы есть у нейронных сетей, которые часто токенизируют странным для нас способом (метод BPE). Мы с вами их смотреть не будем, но в свете наличия этого метода важно помнить, что токен &ndash; это не слово и не знак пунктуации, а \"значимая для анализа последовательность символов\". \n",
    "\n",
    "Качество токенизаторов вы должны были анализировать в прошлом семестре; по скорости лидирует, как обычно, razdel. spacy &ndash; самый сложный в использовании. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a245bc-429e-45c1-aef7-d4043139282a",
   "metadata": {
    "id": "67a245bc-429e-45c1-aef7-d4043139282a",
    "outputId": "710bbd80-f900-48a2-8178-b19eec1b11ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Пердикка', 'II', '(', 'др.-греч', '.', 'Περδίκκας', 'Β΄', 'της', 'Μακεδονίας', ')', '—', 'македонский', 'царь', ',', 'правивший', 'в', '454—413', 'годах', 'до', 'н.', 'э', '.']\n",
      "['После', 'смерти', 'Александра', 'I', 'среди', 'его', 'сыновей', 'возник', 'междоусобный', 'конфликт', ',', 'победителем', 'из', 'которого', 'вышел', 'Пердикка', '.']\n",
      "['На', 'момент', 'его', 'воцарения', 'Македония', 'представляла', 'собой', 'отсталое', 'государство', ',', 'которому', 'угрожала', 'опасность', 'завоевания', 'как', 'со', 'стороны', 'Афинского', 'морского', 'союза', 'на', 'юге', ',', 'так', 'и', 'Одрисского', 'царства', 'на', 'севере', '.']\n",
      "['На', 'первых', 'порах', 'Пердикка', 'был', 'вынужден', 'всеми', 'силами', 'избегать', 'открытого', 'вооружённого', 'противостояния', 'и', 'лишь', 'наблюдать', 'за', 'появлением', 'множества', 'греческих', 'колоний', 'на', 'своих', 'границах', '.']\n",
      "['С', 'началом', 'Пелопоннесской', 'войны', 'македонский', 'царь', 'с', 'максимальной', 'выгодой', 'для', 'государства', 'использовал', 'запутанные', 'отношения', 'между', 'греческими', 'полисами', 'на', 'Халкидиках', ',', 'Афинами', ',', 'Спартой', 'и', 'Коринфом', '.']\n",
      "['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize  \n",
    "\n",
    "for sent in sents:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad5ac2-e327-43eb-90a6-f39751db92ad",
   "metadata": {
    "id": "2dad5ac2-e327-43eb-90a6-f39751db92ad",
    "outputId": "c221e2f4-7044-43c2-87ac-6b4f021ec335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Пердикка', 'II', '(', 'др', '.', '-', 'греч', '.', 'Περδίκκας', 'Β΄', 'της', 'Μακεδονίας', ')', '—', 'македонский', 'царь', ',', 'правивший', 'в', '454—413', 'годах', 'до', 'н', '.', 'э', '.']\n",
      "['После', 'смерти', 'Александра', 'I', 'среди', 'его', 'сыновей', 'возник', 'междоусобный', 'конфликт', ',', 'победителем', 'из', 'которого', 'вышел', 'Пердикка', '.']\n",
      "['На', 'момент', 'его', 'воцарения', 'Македония', 'представляла', 'собой', 'отсталое', 'государство', ',', 'которому', 'угрожала', 'опасность', 'завоевания', 'как', 'со', 'стороны', 'Афинского', 'морского', 'союза', 'на', 'юге', ',', 'так', 'и', 'Одрисского', 'царства', 'на', 'севере', '.']\n",
      "['На', 'первых', 'порах', 'Пердикка', 'был', 'вынужден', 'всеми', 'силами', 'избегать', 'открытого', 'вооружённого', 'противостояния', 'и', 'лишь', 'наблюдать', 'за', 'появлением', 'множества', 'греческих', 'колоний', 'на', 'своих', 'границах', '.']\n",
      "['С', 'началом', 'Пелопоннесской', 'войны', 'македонский', 'царь', 'с', 'максимальной', 'выгодой', 'для', 'государства', 'использовал', 'запутанные', 'отношения', 'между', 'греческими', 'полисами', 'на', 'Халкидиках', ',', 'Афинами', ',', 'Спартой', 'и', 'Коринфом', '.']\n",
      "['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.']\n"
     ]
    }
   ],
   "source": [
    "from razdel import tokenize\n",
    "\n",
    "for sent in sents:\n",
    "    print([t.text for t in tokenize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828efda5-40de-42d9-a00d-faf8b9e3db30",
   "metadata": {
    "id": "828efda5-40de-42d9-a00d-faf8b9e3db30",
    "outputId": "5c1a46b3-2e28-4f3d-f6e4-04ebf57c7016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Пердикка', 'II', '(', 'др.-греч', '.', 'Περδίκκας', 'Β΄', 'της', 'Μακεδονίας', ')', '—', 'македонский', 'царь', ',', 'правивший', 'в', '454—413', 'годах', 'до', 'н', '.', 'э', '.']\n",
      "['После', 'смерти', 'Александра', 'I', 'среди', 'его', 'сыновей', 'возник', 'междоусобный', 'конфликт', ',', 'победителем', 'из', 'которого', 'вышел', 'Пердикка', '.']\n",
      "['На', 'момент', 'его', 'воцарения', 'Македония', 'представляла', 'собой', 'отсталое', 'государство', ',', 'которому', 'угрожала', 'опасность', 'завоевания', 'как', 'со', 'стороны', 'Афинского', 'морского', 'союза', 'на', 'юге', ',', 'так', 'и', 'Одрисского', 'царства', 'на', 'севере', '.']\n",
      "['На', 'первых', 'порах', 'Пердикка', 'был', 'вынужден', 'всеми', 'силами', 'избегать', 'открытого', 'вооружённого', 'противостояния', 'и', 'лишь', 'наблюдать', 'за', 'появлением', 'множества', 'греческих', 'колоний', 'на', 'своих', 'границах', '.']\n",
      "['С', 'началом', 'Пелопоннесской', 'войны', 'македонский', 'царь', 'с', 'максимальной', 'выгодой', 'для', 'государства', 'использовал', 'запутанные', 'отношения', 'между', 'греческими', 'полисами', 'на', 'Халкидиках', ',', 'Афинами', ',', 'Спартой', 'и', 'Коринфом', '.']\n",
      "['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('ru_core_news_md')  \n",
    "# прежде чем делать с текстом что угодно, мы должны загрузить модель для русского языка\n",
    "\n",
    "for sent in sents:\n",
    "    doc = nlp(sent)  \n",
    "    # обрабатываем наш \"текст\" моделью \n",
    "    print([token.text for token in doc])  # принцип примерно такой же, как в razdel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d25449-c983-4274-9784-220d8b9d9d31",
   "metadata": {
    "id": "91d25449-c983-4274-9784-220d8b9d9d31",
    "tags": []
   },
   "source": [
    "#### Стемминг и морфопарсинг\n",
    "\n",
    "Почти все следующие инструменты в питоне устроены довольно однообразно: имеется основной класс \"парсер\", экземпляр которого вам нужно создать, прежде чем что-то парсить. То есть, условно говоря, из набора машинок берете ту конкретную, которая вас повезет. Когда создан экземпляр класса, ему уже можно скармливать свои тексты. \n",
    "\n",
    "Стемминг &ndash; это уже чисто историческое, можно сказать, явление: в 1980-х, когда еще не было даже графического интерфейса у компьютеров и тем более средств автоматического морфоразбора, Мартин Портер разработал свой алгоритм стемминга: усечение окончания от псевдоосновы. Этот алгоритм так и называется \"стеммер Портера\" и доступен в версиях для нескольких европейских языков, в т.ч. для русского (Snowball &ndash; чуть более новая версия). Алгоритм с помощью правил отсекает окончания и суффиксы, основываясь на особенностях языка. Как все правиловое, работает не без ошибок. \n",
    "\n",
    "Код просто посмотреть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4ccfb-99ff-4b7c-ac30-66a6730368dd",
   "metadata": {
    "id": "dbe4ccfb-99ff-4b7c-ac30-66a6730368dd",
    "outputId": "160c8bd6-aaaa-4ff5-aea6-54c2e8d1a761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пердикк\n",
      "не\n",
      "мен\n",
      "десят\n",
      "раз\n",
      "заключа\n",
      "и\n",
      "расторга\n",
      "союз\n",
      "с\n",
      "основн\n",
      "участник\n",
      "войн\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')  # экземпляр класса \n",
    "example = ['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.']\n",
    "for token in example:\n",
    "    print(stemmer.stem(token))  # stem() - метод класса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f005a-cee6-4c1c-9262-21912a279ea2",
   "metadata": {
    "id": "ee2f005a-cee6-4c1c-9262-21912a279ea2"
   },
   "source": [
    "Самые простые &ndash; правиловые морфопарсеры. Для русского языка их два: pymorphy2 и pymystem3. Pymorphy был создан Михаилом Коробовым (вот его известная [статья на хабре](https://habr.com/ru/post/176575/)) как аналог Майстем. Он работает на словаре и использует тагсет [OpenCorpora](http://opencorpora.org/)), а также статистику, предпосчитанную на этом корпусе. \n",
    "Как устроен pymorphy2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4287fd46-d454-4afe-a204-867bdbfbb1d7",
   "metadata": {
    "id": "4287fd46-d454-4afe-a204-867bdbfbb1d7",
    "outputId": "05f2bc90-c3a2-42d6-9212-c1c709b9fb5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='десяти', tag=OpencorporaTag('NUMR gent'), normal_form='десять', score=0.925, methods_stack=((DictionaryAnalyzer(), 'десяти', 930, 1),)),\n",
       " Parse(word='десяти', tag=OpencorporaTag('NUMR datv'), normal_form='десять', score=0.0375, methods_stack=((DictionaryAnalyzer(), 'десяти', 930, 2),)),\n",
       " Parse(word='десяти', tag=OpencorporaTag('NUMR loct'), normal_form='десять', score=0.0375, methods_stack=((DictionaryAnalyzer(), 'десяти', 930, 5),))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "parse = morph.parse('десяти')\n",
    "parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd88d0-a8f6-461d-979b-28234bc2ae5a",
   "metadata": {
    "id": "1fcd88d0-a8f6-461d-979b-28234bc2ae5a"
   },
   "source": [
    "У класса MorphAnalyzer() есть метод parse, который возвращает что? Список экземпляров класса Parse. У этого класса есть свои атрибуты: word (исходная форма слова), tag (грам. инфа), normal_form (лемма), score(предпосчитанная на OpenCorpora вероятность правильности разбора) и несколько технических. \n",
    "\n",
    "Соответственно, получить информацию можно, просто обращаясь к атрибутам (не забудьте, что у нас всегда список, поэтому нужно еще и индекс разбора указывать):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23810a84-aefc-4b7a-9423-f15bd415eacd",
   "metadata": {
    "id": "23810a84-aefc-4b7a-9423-f15bd415eacd",
    "outputId": "390084fa-766a-4c0e-8437-a3361f2709a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "десяти\n",
      "NUMR gent\n",
      "десять\n"
     ]
    }
   ],
   "source": [
    "print(parse[0].word)\n",
    "print(parse[0].tag)\n",
    "print(parse[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e2781-f492-419f-9b72-efee3b920399",
   "metadata": {
    "id": "425e2781-f492-419f-9b72-efee3b920399"
   },
   "source": [
    "Атрибут tag &ndash; это экземпляр класса OpencorporaTag, как можно догадаться. У него есть еще свои атрибуты, к которым тоже можно обращаться, чтобы получать более конкретную информацию о слове. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c56c23-193b-4d59-9b8c-3773887aefda",
   "metadata": {
    "id": "91c56c23-193b-4d59-9b8c-3773887aefda",
    "outputId": "83ea7834-6b67-49a4-fd44-54918b9564da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: NOUN\n",
      "Одушевленность: anim\n",
      "Падеж: nomn\n",
      "Род: masc\n",
      "Наклонение: None\n",
      "Число: sing\n",
      "Лицо: None\n",
      "Время: None\n",
      "Переходность: None\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('участник')\n",
    "\n",
    "t = parse[0].tag  # я записала в переменную, просто чтобы не копировать каждый раз все целиком\n",
    "# но это то же самое, что parse[0].tag.animacy...\n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\\n",
    "\\nНаклонение: {t.mood}\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e994694-8972-41e3-a6aa-2166db361f78",
   "metadata": {
    "id": "2e994694-8972-41e3-a6aa-2166db361f78",
    "outputId": "8fa17186-8f21-47e3-cc8f-7e6c50772e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: VERB\n",
      "Одушевленность: None\n",
      "Падеж: None\n",
      "Род: None\n",
      "Наклонение: indc\n",
      "Число: sing\n",
      "Лицо: 3per\n",
      "Время: pres\n",
      "Переходность: tran\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('говорит')\n",
    "\n",
    "t = parse[0].tag  \n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\n\\\n",
    "Наклонение: {t.mood}\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aad385-5a6e-4a24-91b1-ac66ea2e29b4",
   "metadata": {
    "id": "c3aad385-5a6e-4a24-91b1-ac66ea2e29b4"
   },
   "source": [
    "Если вы запрашиваете категорию, которой у данного слова нет (ну нет переходности у существительного), вернется None. \n",
    "\n",
    "Также можно попросить pymorphy поставить слово в конкретную форму или вообще вернуть всю парадигму. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0545d-c973-4573-9761-219072e229ea",
   "metadata": {
    "id": "59d0545d-c973-4573-9761-219072e229ea",
    "outputId": "3597bfef-8327-499c-bb65-fd1e9cd2ff96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='говорят', tag=OpencorporaTag('VERB,impf,tran plur,3per,pres,indc'), normal_form='говорить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'говорят', 415, 6),))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].inflect({'plur'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3bc211-cfc5-44ce-8b50-bb6fac285336",
   "metadata": {
    "id": "8c3bc211-cfc5-44ce-8b50-bb6fac285336",
    "outputId": "1af4201a-a10c-4480-91e7-feee440916ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='участник', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участник', 2, 0),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 1),)),\n",
       " Parse(word='участнику', tag=OpencorporaTag('NOUN,anim,masc sing,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнику', 2, 2),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 3),)),\n",
       " Parse(word='участником', tag=OpencorporaTag('NOUN,anim,masc sing,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участником', 2, 4),)),\n",
       " Parse(word='участнике', tag=OpencorporaTag('NOUN,anim,masc sing,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнике', 2, 5),)),\n",
       " Parse(word='участники', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участники', 2, 6),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 7),)),\n",
       " Parse(word='участникам', tag=OpencorporaTag('NOUN,anim,masc plur,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участникам', 2, 8),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 9),)),\n",
       " Parse(word='участниками', tag=OpencorporaTag('NOUN,anim,masc plur,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниками', 2, 10),)),\n",
       " Parse(word='участниках', tag=OpencorporaTag('NOUN,anim,masc plur,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниках', 2, 11),))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].lexeme  \n",
    "# парадигму глагола лучше не выводить - она длинная; я перед запуском этой ячейки перезапустила разбор с существительным, поэтому не удивляйтесь. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2436323-ab1c-49a8-b0ce-a86c14b9fcc9",
   "metadata": {
    "id": "a2436323-ab1c-49a8-b0ce-a86c14b9fcc9"
   },
   "source": [
    "Наконец, можно попросить pymorphy выводить грам. информацию по-русски:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b1992-8762-4cf0-98f7-ad8da43ba9c0",
   "metadata": {
    "id": "721b1992-8762-4cf0-98f7-ad8da43ba9c0",
    "outputId": "cc4d0f32-2b8a-49e3-801e-1f44c2938d53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'СУЩ,од,мр ед,им'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].tag.cyr_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06822f-658c-403a-b063-9562578d2d9f",
   "metadata": {
    "id": "0a06822f-658c-403a-b063-9562578d2d9f"
   },
   "source": [
    "Pymorphy очень быстро работает и имеет много возможностей, но совершенно не умеет разрешать омонимию и никак не учитывает контекст. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8f0aa-7e2f-424b-9b23-c3563a7540a8",
   "metadata": {
    "id": "c3c8f0aa-7e2f-424b-9b23-c3563a7540a8"
   },
   "source": [
    "Алгоритм, легший в основу Mystem, разрабатывался в ИППИ и был первым вообще для русского языка; его в свое время купил у них Илья Сегалович, доработал, опубликовал собственную статью. Поисковик Яндекса когда-то работал на майстеме. Сам парсер написан в С (для скорости: бинарный поиск в питоне реализовать можно только с внешними библиотеками на С, а у майстема 2 словаря, по которым нужно искать). Для питона под него сделана оболочка (pymystem3). Майстем капризный, тяжело запускается, имеет не так много функций, но работает тоже довольно быстро и умеет доносить на бастардов: сообщать, что слово не найдено в его словаре. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacff22d-ddf5-475d-97c6-8dfdf3cd782e",
   "metadata": {
    "id": "bacff22d-ddf5-475d-97c6-8dfdf3cd782e"
   },
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "\n",
    "m = pymystem3.Mystem(entire_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c43ab-edb0-4454-9ded-51a365479776",
   "metadata": {
    "id": "a35c43ab-edb0-4454-9ded-51a365479776"
   },
   "source": [
    "Майстем принимает только сырой текст в виде одной строки: у него встроенный токенизатор, потому что он пытается учитывать контекст. Поэтому стоит указывать entire_input=False при создании экземпляра класса, чтобы он не выводил вообще все, включая пробелы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e2812-2ac6-4597-8f85-f5861246a199",
   "metadata": {
    "id": "fe8e2812-2ac6-4597-8f85-f5861246a199"
   },
   "outputs": [],
   "source": [
    "lemmas = m.lemmatize(raw)\n",
    "analysis = m.analyze(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c5f88-1e17-4294-8ffe-72591f82d3e5",
   "metadata": {
    "id": "f88c5f88-1e17-4294-8ffe-72591f82d3e5",
    "outputId": "cd0ecffe-104e-467e-e8a8-789a0c295a04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['пердикк',\n",
       " 'II',\n",
       " 'др',\n",
       " 'греча',\n",
       " 'Περδίκκας',\n",
       " 'Β',\n",
       " 'της',\n",
       " 'Μακεδονίας',\n",
       " 'македонский',\n",
       " 'царь']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[:10]  # надеюсь, греча вас тоже порадовала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d702a-4b3e-4e8a-aaa4-ee81216ebca5",
   "metadata": {
    "id": "e70d702a-4b3e-4e8a-aaa4-ee81216ebca5",
    "outputId": "17f714ee-8947-4f94-9c32-7ad81d7b791e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'пердикк',\n",
       "    'qual': 'bastard',\n",
       "    'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}],\n",
       "  'text': 'Пердикка'},\n",
       " {'analysis': [], 'text': 'II'},\n",
       " {'analysis': [{'lex': 'др', 'gr': 'S,сокр,мн,неод=(пр|вин|дат|род|твор|им)'}],\n",
       "  'text': 'др'},\n",
       " {'analysis': [{'lex': 'греча', 'gr': 'S,жен,неод=род,мн'}], 'text': 'греч'},\n",
       " {'analysis': [], 'text': 'Περδίκκας'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c650a8b-240d-4e1f-92de-ea00b8030c07",
   "metadata": {
    "id": "8c650a8b-240d-4e1f-92de-ea00b8030c07"
   },
   "source": [
    "С леммами вроде все должно быть понятно, а что зашито в анализе?\n",
    "\n",
    "Майстем возвращает список. Каждый токен в этом списке - это словарь с ключами analysis & text. Первого ключа может не быть: если у нас знак пунктуации. Если же он есть, то в нем содержится список (обычно состоящий из одного элемента - если не указать при создании экземпляра класса Mystem glue_grammar_info=False). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770399d-9119-4642-a543-87bfddf097d4",
   "metadata": {
    "id": "8770399d-9119-4642-a543-87bfddf097d4",
    "outputId": "b9618f2b-07d8-43aa-bd62-e605466b6e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое слово: {'analysis': [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}], 'text': 'Пердикка'}\n",
      "Его грам. инфа: [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}]\n",
      "Его оригинальная форма: Пердикка\n",
      "Какие есть ключи в словаре с разбором: dict_keys(['lex', 'qual', 'gr'])\n",
      "Лемма: пердикк\n",
      "Этот ключ бывает только тогда, когда слова нет в словаре: bastard\n",
      "А это грам. информация: S,имя,муж,од=(вин,ед|род,ед)\n"
     ]
    }
   ],
   "source": [
    "print(f'Первое слово: {analysis[0]}')\n",
    "print(f\"Его грам. инфа: {analysis[0]['analysis']}\\nЕго оригинальная форма: {analysis[0]['text']}\")\n",
    "print(f\"Какие есть ключи в словаре с разбором: {analysis[0]['analysis'][0].keys()}\")\n",
    "print(f\"Лемма: {analysis[0]['analysis'][0]['lex']}\\n\\\n",
    "Этот ключ бывает только тогда, когда слова нет в словаре: {analysis[0]['analysis'][0]['qual']}\\n\\\n",
    "А это грам. информация: {analysis[0]['analysis'][0]['gr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d17cff-8ce3-47d4-b641-e3d6fd0887c9",
   "metadata": {
    "id": "c8d17cff-8ce3-47d4-b641-e3d6fd0887c9"
   },
   "source": [
    "Непросто, да. Еще сложнее устроен ключ 'gr', который содержит грамматическую информацию о слове: обычно майстем склеивает варианты разбора, то есть, выше запись следует читать как \"существительное, имя собственное, мужского рода, одушевленное; возможно, Acc Sg, а возможно, Gen Sg. \n",
    "\n",
    "Вот как раз если указать, чтобы грам. информация не склеивалась, майстем будет возвращать несколько словарей с вариантами по отдельности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70484d-e134-4edb-bd74-38900f913bc0",
   "metadata": {
    "id": "bd70484d-e134-4edb-bd74-38900f913bc0",
    "outputId": "ecaf02dc-48d7-4c1d-e1fc-0ddd6bd32e1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysis': [{'lex': 'пердикк',\n",
       "   'qual': 'bastard',\n",
       "   'gr': 'S,имя,муж,од=вин,ед'},\n",
       "  {'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=род,ед'}],\n",
       " 'text': 'Пердикка'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_noglue = pymystem3.Mystem(entire_input=False, glue_grammar_info=False)\n",
    "\n",
    "noglueanalysis = m_noglue.analyze(raw)\n",
    "noglueanalysis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe4348-1c94-4a9c-9dee-bea45c42680c",
   "metadata": {
    "id": "7abe4348-1c94-4a9c-9dee-bea45c42680c"
   },
   "source": [
    "Теперь о вещах, которых нет в стабильной версии Mystem, а есть только в той, которая устанавливается через git:\n",
    "\n",
    "1. Майстем очень плохо умеет обрабатывать \\n. Когда он получает строку, в которой много \\n (а это неизбежно, ведь мы чаще хотим обрабатывать длиннющие тексты), на каждом \\n он перезапускает свой бинарник (написанный в С). Поэтому на длинных текстах работать будет ОЧЕНЬ медленно (впрочем, все равно быстрее нейронок...). Чтобы решить эту проблему - ведь замена \\n на пробелы, например, искажает контекст - сделали возможность особым образом обрабатывать \\n, когда загружаем текст из файла. \n",
    "2. Есть функция, которая позволяет получить часть речи для конкретного токена. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c21f02-f5c7-4053-bddf-d9b66257651c",
   "metadata": {
    "id": "54c21f02-f5c7-4053-bddf-d9b66257651c"
   },
   "outputs": [],
   "source": [
    "analyze = m.analyze(file_path=path) # можно напрямую передавать в майстем путь к файлу с текстом - он сам откроет и обработает как ему надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404a565f-463f-4737-a7aa-4be08a724eff",
   "metadata": {
    "id": "404a565f-463f-4737-a7aa-4be08a724eff",
    "outputId": "4134514f-33d4-4db3-d241-7e316d0f4e7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_pos(analysis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ece345-21ff-419d-9062-e899d96d5991",
   "metadata": {
    "id": "19ece345-21ff-419d-9062-e899d96d5991"
   },
   "source": [
    "В 2017 году на соревновании конференции \"Диалог\" победила команда Гусев-Анастасьев: ребята создали морфопарсер для русского языка на нейронной сети. Он называется rnnmorph (RNN - это название модели нейронной сети, на которой он работает). Гусев при создании явно вдохновлялся pymorphy2 (кстати, Коробов для этого же соревнования сделал библиотеку для приведения разных тагсетов к одному): синтаксис rnnmorph очень похож на pymorphy2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8abc864f-3182-4d6f-adac-c1f3a70baf26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8abc864f-3182-4d6f-adac-c1f3a70baf26",
    "outputId": "b974686e-efbe-4b71-ec6c-36e6087866b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<normal_form=пердикк; word=Пердикка; pos=NOUN; tag=Case=Acc|Gender=Masc|Number=Sing; score=0.7922>,\n",
       " <normal_form=не; word=не; pos=PART; tag=_; score=1.0000>,\n",
       " <normal_form=менее; word=менее; pos=ADV; tag=Degree=Cmp; score=1.0000>,\n",
       " <normal_form=десять; word=десяти; pos=NUM; tag=Case=Gen; score=1.0000>,\n",
       " <normal_form=раз; word=раз; pos=NOUN; tag=Case=Gen|Gender=Masc|Number=Plur; score=0.9998>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "\n",
    "predictor = RNNMorphPredictor(language='ru')\n",
    "parse = predictor.predict(['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.'])\n",
    "parse[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb9996-e5eb-4645-9506-58a88240cd78",
   "metadata": {
    "id": "wgoJen3T56Q3"
   },
   "source": [
    "rnnmorph принимает список строк (токенов) и возвращает список объектов специального класса, у которого есть атрибуты normal_form (лемма), word (исходная форма), pos (часть речи), tag (грам. информация) и score (уверенность нейронной модели в правильности своего ответа). Он умеет снимать омонимию (лучше, чем майстем, но хуже, чем интегральный морфопарсер). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "232755ea-ad85-4479-91fe-fbd1efbed691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[-2].pos  # например, можно узнать часть речи для предпоследнего слова в списке"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3f964-15df-42eb-9260-1447b147ce9b",
   "metadata": {},
   "source": [
    "*Примечание*\n",
    "\n",
    "RNNMorph, как и любая нейронная модель, умеет работать на GPU. \n",
    "\n",
    "Для любопытных: нейронные сети &ndash; это, по сути, бесконечное умножение гигантских матриц друг на друга. Нейронная сеть состоит из кучи нейронов-функций, и когда она должна выдать ответ, она получает входные данные в виде таких же матриц на первый слой, который состоит из миллиона нейронов, этот миллион нейрончиков кидается высчитывать результат зашитой внутри них функции (что-то похожее на ax + b), а их ответы получает второй слой таких же нейрончиков, и так пока не получится финальный ответ. То есть, мы производим миллиарды однотипных вычислений. Процессор видеокарты устроен как раз таким образом, что он супер-быстро умеет считать однотипные вещи (он считает их батчами &ndash; сразу пачками), то есть, он работает гораздо быстрее, чем CPU, но обучен именно однотипно считать. Поэтому нейронки обычно и работают на GPU, они просто созданы друг для друга!\n",
    "\n",
    "Когда нейронка (и RNNMorph тоже) работает на видеокарте, она делает это обычно быстрее, чем на CPU. Чтобы заставить у себя локально RNNMorph работать на видеокарте, правда, придется долго сложно настраивать cuda (и это еще если у вас NVidia). Но можно и без видеокарты, просто тогда tensorflow, на котором работает нейронка, будет выдавать предупреждения о том, что он будет использовать медленный CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6b9c3-6653-46ef-84e4-1fa623667b3c",
   "metadata": {},
   "source": [
    "Последний парсер, который мы посмотрели &ndash; это интегральный морфопарсер Анастасьева, победивший в соревнованиях в 2020 году. Его особенность в том, что он работает в формате Universal Dependencies и делает одновременно морфо- и синтаксический разбор. Поэтому, прежде чем перейти к нему, посмотрим библиотеку для просмотра файлов формата .conllu &ndash; в нем записывается UD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e29c5c-9826-400b-894b-0459860e6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll\n",
    "\n",
    "text = pyconll.load_from_file('myfilem.conllu')\n",
    "\n",
    "for sentence in text:\n",
    "    for token in sentence:\n",
    "        print(token.id, token.form, token.lemma, token.upos, token.feats, token.head, token.deprel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa00e9d-18eb-4418-9fce-b47e52b5bc5c",
   "metadata": {},
   "source": [
    "Я не буду выводить получившееся, а то будет слишком длинно. Естественно, можно не только печатать информацию, но и добавлять в список и вообще делать все, что угодно. Что это за атрибуты у токенов?\n",
    "\n",
    "- id - порядковый номер токена в предложении\n",
    "- form - исходная форма\n",
    "- lemma - лемма\n",
    "- upos - часть речи \n",
    "- feats - грам. характеристики\n",
    "- head - расстояние от синтаксической вершины\n",
    "- deprel - тип синтаксической связи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92010839-c9e2-48d0-b7f7-3ff18ef3a7f4",
   "metadata": {},
   "source": [
    "Где можно красивенько отрисовать .conllu файлы в виде деревьев зависимости:\n",
    "\n",
    "[Арборатор](https://arborator.ilpga.fr/q.cgi): достаточно вставить текст в формате .conllu\n",
    "\n",
    "[Conllu-Viewer на сайте UD](https://universaldependencies.org/conllu_viewer.html): умеет читать файлы и рисовать последовательно все предложения\n",
    "\n",
    "Для затравки вот картинка с арборатора:\n",
    "\n",
    "<img src='arbo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268812e-4693-461b-8910-68c242d0dd54",
   "metadata": {},
   "source": [
    "Сам парсер Анастасьев, к сожалению, так и не собрал в устанавливаемый модуль, но простые лингвисты могут воспользоваться его тетрадкой (я ее дорабатывала немного, потому что совместимости со временем страдают): [Колаб с интегральным морфопарсером](https://colab.research.google.com/drive/1nuIqJ-YUuDihSzi37A-YLy2uW7Ll-2Rr?usp=sharing)\n",
    "\n",
    "(Скопируйте эту тетрадку себе). \n",
    "\n",
    "Как ей пользоваться?\n",
    "\n",
    "1. Запустить ячейку Dependencies. \n",
    "2. Пока она выполняется, можно успеть подготовить свой пустой .conllu для заполнения... Бдите, чтобы не было слишком длинных предложений!\n",
    "3. Загрузить свой файл в любую папку в колаб. \n",
    "4. Указать путь к этой папке в ячейке Apply: \n",
    "\n",
    "        !cd GramEval2020 \\\n",
    "            && ./download_model.sh ru_bert_final_model \\\n",
    "            && cd solution \\\n",
    "            && python -m train.applier --data-dir /content/test --model-name ru_bert_final_model --batch-size 8 --pretrained-models-dir ../pretrained_models\n",
    "            \n",
    "       Здесь вместо /content/test должно быть имя вашей папки (/content - это сама директория, поэтому если заводите новую папку, то будет /content/ваша папка)\n",
    "       \n",
    "5. Можно дополнительно выбрать другую модель из списка в [гитхабе Анастасьева](https://github.com/DanAnastasyev/GramEval2020), но они не все рабочие. \n",
    "6. Запустить Apply! \n",
    "\n",
    "Если начнут вываливаться ошибки, гуглите: могли полететь совместимости. Если выдает ошибку про \"мало памяти, дай ищо\", проверьте свой файл на длинные предложения!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "lec06.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
