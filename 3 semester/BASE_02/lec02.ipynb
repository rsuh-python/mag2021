{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой тетрадке я только добавила комментарии к коду из семинара. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWfmZei5t_8I"
   },
   "source": [
    "### natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка natasha ничего особенного не требует, просто выполните команду pip install natasha в командной строке / терминале. Наташа - библиотека NLP-инструментов только для русского языка, она очень быстрая, потому что многие вещи в ней работают на правилах. У Наташи есть несколько отдельных подбиблиотек, мы с вами уже знаем razdel. \n",
    "\n",
    "В отличие от spacy и других подобных библиотек-комбайнов, Наташа составная: вам придется вызывать по отдельности те процессоры для текста (морфологический, синтаксический...), которые вам нужны. Если не вызовете тот, без которого не работает нужный вам, не сработает тот, что вам нужен. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fw52LhAct9Tg"
   },
   "outputs": [],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VqX_VxouI-q"
   },
   "source": [
    "Морфосинтаксический парсинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы должны импортировать из Наташи несколько классов: \n",
    "\n",
    "- Segmenter - для сегментации (токенизация и деление на предложения): без этого процессора, скорее всего, ни один другой нормально не отработает\n",
    "- NewsEmbedding - эмбеддинги слов, то есть, векторные представления слов. Без эмбеддингов не работают морфо- и синтаксический парсеры, NER-тэггер. \n",
    "- NewsMorphTagger - морфопарсер\n",
    "- NewsSyntaxParser - синтаксический парсер\n",
    "- Doc - класс для хранения и обработки нашего текста\n",
    "\n",
    "Также потом будем импортировать еще эти:\n",
    "\n",
    "- NewsNERTagger - разметчик именованных сущностей\n",
    "- MorphVocab - достает леммы из морфоразбора (без него не работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1663154496060,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "S5k9Wlz9uK7M"
   },
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    \n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciOkGldHuLgB"
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter()  # токенизация и разделение на предложения\n",
    "emb = NewsEmbedding()  # эмбеддинги\n",
    "morph_tagger = NewsMorphTagger(emb)  # морфология. Мы должны передать в морфопарсер эмбеддинги, потому что без них он не работает\n",
    "syntax_parser = NewsSyntaxParser(emb) # синтаксис\n",
    "\n",
    "text = '29 марта 2017 года правительство Великобритании инициировало процедуру выхода в соответствии со статьёй 50 Договора о Европейском союзе; первоначально планировалось, что Великобритания покинет Европейский союз через два года, 29 марта 2019 года в 23:00 по Гринвичу.'\n",
    "doc = Doc(text)\n",
    "\n",
    "doc.segment(segmenter)  # сегментируем наш текст\n",
    "doc.tag_morph(morph_tagger)  # морфопарсим его\n",
    "doc.parse_syntax(syntax_parser)  # и синтаксис разбираем \n",
    "sent = doc.sents[0]  # в атрибуте doc.sents хранится итератор со всеми нашими предложениями; под индексом 0 - первое предложение\n",
    "sent.morph.print()  # внимательно: здесь print не функция, а метод для специализированного наташиного красивого вывода\n",
    "sent.syntax.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPCVWdB1ujFp"
   },
   "source": [
    "Распознание именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaTUTjyMuikI"
   },
   "outputs": [],
   "source": [
    "from natasha import NewsNERTagger\n",
    "\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "doc.tag_ner(ner_tagger)\n",
    "doc.ner.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Log6uopyux7n"
   },
   "source": [
    "Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNSGZViPuuJd"
   },
   "outputs": [],
   "source": [
    "from natasha import MorphVocab\n",
    "\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "for token in doc.tokens:\n",
    "  token.lemmatize(morph_vocab)\n",
    "  print(token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1dqpS_Hu3OT"
   },
   "source": [
    "Нормализация именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqmxRFqbu2m8"
   },
   "outputs": [],
   "source": [
    "for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "   \n",
    "{_.text: _.normal for _ in doc.spans}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg1ZZ8BAxM6g"
   },
   "source": [
    "Упражнения\n",
    "\n",
    "- Возьмите любой достаточно длинный новостной текст, извлеките из него все именованные сущности и нормализуйте их. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BplzY__-xWb4"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcb7BQ4BxYIn"
   },
   "source": [
    "- Постройте дерево зависимостей для одного и того же предложения в natasha и spacy или corpy (придется вручную записать файл .conll и отправить его содержимое в арборатор, например: как это сделать, уточните в лекции 6 за прошлый семестр). Сравните, что вам больше нравится. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idrup3gKyB8C"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GU9unbnvJXP"
   },
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эту библиотеку разрабатывают ученые из Стэнфордского университета (первоначально она называлась StanfordNLP и была написана в Java). Она тоже довольно популярная и, как и spacy, по сути является только общей оболочкой для разных нейронных моделек, к которым просто предоставляет интерфейс. Сами модельки, если вы заметили, скачиваются с известного сайта huggingface.co, где выкладываются обученные нейронные сети и датасеты. \n",
    "\n",
    "Станца похожа на spacy и для морфосинтаксиса включает в себя те же модели UDPipe. \n",
    "\n",
    "При установке может возникнуть проблема, если у вас уже был случайно установлен модуль pytorch не той версии; проверьте с помощью команды pip list, есть ли у вас pytorch 1.16 в списке, если нет - то все хорошо и можно устанавливать станцу, а если есть, то стоит его удалить pip uninstall pytorch. Если есть torch 1.12, то это ок. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKwCF6UOvRCh"
   },
   "outputs": [],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv7BrtxEvUwY"
   },
   "source": [
    "Загрузка моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели станца загружает автоматически, когда вы создаете объект класса Pipeline. Обязательный аргумент для создания такого класса - только аббревиатура вашего языка (список доступных моделек есть в [документации станцы](https://stanfordnlp.github.io/stanza/available_models.html)). Необязательный атрибут - processors, куда можно в строке списком передать все процессоры, которые вы хотите включить, то есть, те модели, которые вам нужны. Например, в коде ниже я для русского языка включила все имеющиеся в наличии, потому что ничего не указала, для английского включила только токенизацию, морфопарсинг и парсинг составляющих, а для французского - токенизацию и multiword tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2995,
     "status": "ok",
     "timestamp": 1663154625797,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "8UtSgSntvXro"
   },
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7VKLnRkvZd4"
   },
   "outputs": [],
   "source": [
    "nlp_ru = stanza.Pipeline(lang='ru')\n",
    "nlp_en = stanza.Pipeline(lang='en', processors='tokenize, pos, constituency')\n",
    "nlp_fr = stanza.Pipeline(lang='fr', processors='tokenize, mwt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUR7LXjmvtpn"
   },
   "source": [
    "Токенизация, сегментация по предложениям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все остальное работает ровно так же, как в spacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 2242,
     "status": "ok",
     "timestamp": 1663154958031,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "Db8CYc_SvtIX"
   },
   "outputs": [],
   "source": [
    "text = '''Архитектура Византии — совокупность традиций строительства и архитектуры в поздней Римской империи и в Византии в период с начала IV века по середину XV века. В качестве отдельных направлений исследования выделяют религиозную архитектуру Византии, византийскую фортификацию и гражданское строительство, включающее дворцы, общественные сооружения и частные дома. Также в рамках данной дисциплины изучают традиции строительного ремесла и декоративного искусства.'''\n",
    "\n",
    "doc = nlp_ru(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1663154817857,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "MMT0yrtGv3wP",
    "outputId": "2e4e3a15-9345-47c7-ed71-e4c8fc17dc2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Архитектура Византии — совокупность традиций строительства и архитектуры в поздней Римской империи и в Византии в период с начала IV века по середину XV века.\n",
      "В качестве отдельных направлений исследования выделяют религиозную архитектуру Византии, византийскую фортификацию и гражданское строительство, включающее дворцы, общественные сооружения и частные дома.\n",
      "Также в рамках данной дисциплины изучают традиции строительного ремесла и декоративного искусства.\n"
     ]
    }
   ],
   "source": [
    "print(*[sentence.text for sentence in doc.sentences], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше у меня немного сложноватые генераторные выражения, но любое из этих выражений можно на самом деле развернуть в цикл. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxBb4XOMwJdf"
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(doc.sentences):\n",
    "  print(f'====== Sentence {i+1} tokens =======')\n",
    "  print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мультисловная токенизация - это когда, если у нас в языке есть сливающиеся слова, как во французском, токенизатор вернет слова, которые слились:\n",
    "\n",
    "    du = de + le\n",
    "    итал. del = de + il\n",
    "    нем. im = in + dem\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2cRJqFSwO-v"
   },
   "outputs": [],
   "source": [
    "text_fr = '''Il est révélé par les romans Extension du domaine de la lutte (1994) et, surtout, Les Particules élémentaires (1998), qui le fait connaître d'un large public.'''\n",
    "\n",
    "doc_fr = nlp_fr(text_fr)\n",
    "for token in doc_fr.sentences[0].tokens:\n",
    "    print(f'token: {token.text}\\twords: {\", \".join([word.text for word in token.words])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все следующие вещи на самом деле можно объединить в один большой цикл, который будет выводить всю информацию вообще: все, что есть в формате UD, содержится в атрибутах токенов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlS7SKLUwetI"
   },
   "source": [
    "Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh6HPJn2wgcp"
   },
   "outputs": [],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SsW-5MFwupQ"
   },
   "source": [
    "Морфопарсинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2wxiB8NwtYf"
   },
   "outputs": [],
   "source": [
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKZwX5l8wy6Q"
   },
   "source": [
    "Парсинг синтаксических зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w11Jqo6Fwx14"
   },
   "outputs": [],
   "source": [
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7zTW3sJw27A"
   },
   "outputs": [],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-1cXBtWw5K4"
   },
   "source": [
    "Парсинг составляющих (для русского недоступен)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KGb8sV6w6lo"
   },
   "outputs": [],
   "source": [
    "doc_en = nlp_en('This is a sentence for parsing constituencies.')\n",
    "\n",
    "for sentence in doc_en.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmZnXnpiycdY"
   },
   "source": [
    "**Упражнения**\n",
    "\n",
    "Сделайте токенизацию и морфосинтаксический парсинг для одного предложения на любом языке (доступность моделей для вашего языка можно посмотреть [здесь](https://stanfordnlp.github.io/stanza/available_models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmFSXvteys1u"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNltOW1qVSOHhDinlvQSfFt",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
